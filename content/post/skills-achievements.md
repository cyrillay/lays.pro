---
title: "Skills and random Achievements"
date: 2020-04-14T13:15:43+02:00
draft: false
---


To give you an overview of what I can accomplish, here are a few examples of projects and missions I completed over the course of my carreer.


### Data science

#### Computer vision
- Participated in the kaggle deep fake detection challenge : classifying genuine and fake videos (generated by a GAN) from a 500GB dataset. I processed the videos to reduce the dataset size, by first extracting a few frames per video, than performing image segmentation to extract the faces contained in the frames, which happened to contain glitches generated by the GAN. I used fast-ai to train a CNN on those faces and managed to get a 75% accuracy by averaging the prediction on each frame of a video.

- Built a public REST API ([link](../restnet50-classification-rest-api/)) accepting image urls and returning classification labels using a pre-trained neural networks.

#### Keras on Tensorflow - R
- Built high performance models for a farm yield dataset. After a process of feature engineering, hyper parameters tuning and model training, I selected an SVR model.

--- 

### Data Engineering

#### Druid (real time database) : 
- Deployed from end-to-end a [mechanism](https://druid.apache.org/docs/latest/development/extensions-core/lookups-cached-global.html) for joining fact tables and dimension tables (that are constantly updated in another mySQL database) at query time.  
*Impact* : __Saved engineering time : no need to alter the ETL and Spark jobs for every new field required by the buiness. The lookups are now dynamic, and the mappings can change without having to reprocess and backfill historical data.__

#### Kafka
- Deployed monitoring and managment tools for Kafka ([Cruise Control](https://github.com/linkedin/cruise-control) and [CCFE](https://github.com/linkedin/cruise-control-ui) by linkedin) accross multiple Kafka clusters of the company. Dockerized both applications and added CI/CD pipelines with Drone for automated deployment on ECS.  
*Impact* : __Reduced the required engineering time for scaling up/down and rebalancing any kafka clusters by 90%.__

#### Spark
- Switched from RDD operations to use the Dataframe and typed Dataset API, for better performance using Spark Catalyst and Tungsten optimizations
- Added [new configuration](https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-spark-configure.html#emr-spark-maximizeresourceallocation) in EMR for automatically setting spark parameters (memory, parallelism, number of executors, of cores per executor...) depending on the cluster size and hardware.  
*Impact* : __Better performance and divided job duration by 5 on certain jobs, saved AWS cost.__
- Implemented various Spark jobs to predict pedestrian flow at each time on a given google maps address